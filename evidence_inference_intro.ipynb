{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Inference Dataset Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to import the preprocessor file, as this will help with grabbing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from evidence_inference.preprocess.preprocessor import get_Xy, train_document_ids, test_document_ids, validation_document_ids, get_train_Xy\n",
    "\n",
    "tr_ids, val_ids, te_ids = train_document_ids(), validation_document_ids(), test_document_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can view the type of the ids, and what the ids actually consist of (validation ids and test ids are of a similar format). Please note that these are linked to the articles by appending 'PMC' + str(id)  + '.nxml' to find the corresponding xml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object is of type <class 'odict_keys'>\n",
      "The first 10 elements are: [5741844, 3794450, 4759860, 5460737, 5054596, 4786378, 2430617, 5103135, 3410988, 4464926]\n"
     ]
    }
   ],
   "source": [
    "print(\"The object is of type {}\".format(type(tr_ids)))\n",
    "print(\"The first 10 elements are: {}\".format(list(tr_ids)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] We can either use a preset vocab list, or just use nothing. Here, we will use a preset one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_f = os.path.join(\"./annotations\", \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will grab the training data... To get the full article, we don't want any sections_of_interest. We also do not need to know where the sentences are split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2351706 words from vocab file ./annotations\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "## NOTE: This may take a little while to load.\n",
    "train_Xy, inference_vectorizer = get_train_Xy(tr_ids, sections_of_interest=None, vocabulary_file=vocab_f, include_sentence_span_splits = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The types of our outputs are: <class 'list'> (trainXy), and <class 'evidence_inference.preprocess.preprocessor.SimpleInferenceVectorizer'> (inference_vectorizer)\n",
      "\n",
      "trainXy's inner dimension is of type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The types of our outputs are: {} (trainXy), and {} (inference_vectorizer)\\n\".format(type(train_Xy), type(inference_vectorizer)))\n",
    "print(\"trainXy's inner dimension is of type: {}\".format(type(train_Xy[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what one instance of X's keys look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at trainXy's first element's keys:\n",
      " dict_keys(['article', 'I', 'C', 'O', 'a_id', 'p_id', 'y', 'sentence_span', 'evidence_spans'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's look at trainXy's first element's keys:\\n {}\".format(train_Xy[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the validation data, and the test data. They are of a similar format to the training data.\n",
    "val_Xy  = get_Xy(val_ids, inference_vectorizer, sections_of_interest=None, include_sentence_span_splits = False)\n",
    "test_Xy = get_Xy(te_ids, inference_vectorizer, sections_of_interest=None, include_sentence_span_splits = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the X's from a (train/validation/test) data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This isn't exactly what we want for training data though... So, let's pull out some useful bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = [[inner[i] for inner in train_Xy] for i in ['article', 'I', 'C', 'O']]\n",
    "print(\"Training is an array of length {}, w/ inner length {}, such that the ith element of all of the inner arrays are from the same prompt\".format(len(tr_data), len(tr_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some training data... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is what an article looks like: {} ...\".format(tr_data[0][0][:10]))\n",
    "print(\"This is what an outcome looks like: {}\".format(tr_data[1][0]))\n",
    "print(\"This is what an intervention looks like: {}\".format(tr_data[2][0][:10]))\n",
    "print(\"This is what an comparator looks like: {}\".format(tr_data[3][0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Y's from a (train/validation/test) data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what the y looks like for 1 prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Xy[5000]['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define this helper to get us the proper labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def _get_y_vec(y_dict):\n",
    "    # +1 because raw labels are -1, 0, 1 -> 0, 1, 2\n",
    "    # for indexing reasons that appear in the loss function\n",
    "    # (cross-entropy loss wants the index of the highest value, and we index at 0)\n",
    "    all_labels = [y_j[0] + 1 for y_j in y_dict]\n",
    "    y_collapsed = int(stats.mode(all_labels)[0][0])\n",
    "    y_vec = np.zeros(3)\n",
    "    y_vec[y_collapsed] = 1.0\n",
    "    return y_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using our helper function to inspect the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels = [_get_y_vec(inst['y']) for inst in train_Xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is what our labels will look like: {}\".format(tr_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting evidence spans from the (train/validation/test) data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It might also be helpful to work with the evidence spans (i.e. from above: 'Furthermore, the results ...') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_evidence_spans(y_dict, inference_vectorizer):\n",
    "    res = []\n",
    "    for arr in y_dict:\n",
    "        res.append(inference_vectorizer.string_to_seq(arr[-1]))\n",
    "    return res\n",
    "\n",
    "tr_spans = [_get_evidence_spans(inst['y'], inference_vectorizer) for inst in train_Xy]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"This is what an evidence span for a prompt will look like:\\n{}\".format(tr_spans[5000][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have finished going over the essentials, but here are some extra features that may be helpful:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at WHERE in the article the evidence span is given..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_ev_spans = [inst['evidence_spans'] for inst in train_Xy]\n",
    "print(\"For an arbituary prompt, here is where the evidence spans are: {}\".format(where_ev_spans[5000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing our spans with ground truth (at the time of this notebook, it is only 87 percent accuracte (ignoring some encoding issues))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(span_st, span_end) = list(where_ev_spans[5000])[0]\n",
    "# Now let's compare\n",
    "print(\"Here is what it looks like in the article:\\n{}\".format(tr_data[0][5000][span_st:span_end]))\n",
    "print(\"Here is what the evidence span looks like:\\n{}\".format(tr_spans[5000][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
